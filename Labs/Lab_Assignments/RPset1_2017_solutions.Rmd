---
title: 'R Problem Set 1: Loops, Functions, and Matrix Algebra'
author: "Your name goes here"
date: 'Due date: September 12, 2017'
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# SOLUTIONS

#Instructions

Please print and submit a hardcopy of your completed problem set (the knitted .pdf) in class, 9:00AM Tuesday, September 12, 2017. Late submissions will not be accepted. *Note*: please do *not* email files. It will be easier to grade your problem sets if given the hardcopy.

Please do the following problems. When you see a line of code like `set.seed(123)`, just leave it.  It is to make your assignments more easily comparable from student to student.

### How to do this problem set

You will get more out of this if you try the assignment on your own first. If you get stuck, search online for a solution. The best way to do this is to google your error messages, or describe what you're trying to do, along with the letter R.  For instance, "how do I create a data frame in R?" or "Error in \`[.data.frame\`(dat, 4) : undefined columns selected." If you get stuck, team work is permitted, but you must write up your own solutions.  

R code goes in code chunks like this:
```{r}
print("Hello World")
```

They begin with three backticks (the key below the escape key in the upper left part of your keyboard) and {r}. They end with three backticks, so that the markdown compiler knows to go back to printing text, rather than evaluating your code. You may in the future want to pass additional arguments to knitr (the engine that compiles R markdown documents) by including additional arguments in the curly braces. For now, you can use the defaults. A keyboard shortcut for creating a new code chunk is Command-Option-I (Mac) or Control-Alt-I (Windows).

Put your solutions below the questions in this .Rmd file. It is a good practice to periodically compile the document using the "Knit PDF" button. This lets you check on whether the document has any errors in it.  

You should also test your code as you are writing it by pressing Command-Enter (Mac) or Ctrl-Enter (Windows). If you have code selected, it will run that block of code. If you do not have any code selected, it will run the line of code where your cursor is.

---

## Part 1 

### Problem 1: Working with For Loops

The following code will create the initial data frame `dat` for this problem:

```{r}
# Create a vector x and assign to it values from -2 to 2 in increments of 0.1.
x <- seq(from=-2, to=2, by=0.1)
```

```{r}
## Create Data Frame
dat <- as.data.frame(matrix(nrow=length(x), ncol=3))
dat[,1] <- x
dat[,2] <- x + x
dat[,3] <- x * x

colnames(dat) <- c("x", "x.plus", "x.multiply")
```

a) Using a for loop, add a fourth column---"dev"---to your data frame that computes for each row $j$ the average absolute deviation from the mean of each row: $\frac{1}{3}\sum\limits_{j=1}^{3}|x_{ij}-\bar{x}_j|$. 

```{r}
dat[,4] <- NA
for(i in 1:length(dat[,4])){
  data <- dat[i,1:3]
  mean <- (1/3)*sum(data, na.rm=T)
  abs.dev <- abs(data - mean)
  avg <- (1/3)*sum(abs.dev, na.rm=T)
  dat[i,4] <- avg
}

colnames(dat) <- c("x", "x.plus", "x.multiply", "dev")
```


b) Create a vector "months" that contains the first four months of the year.

```{r}
months <- c("January", "February", "March", "April")
```

c) Add a fifth column---"month"---to your data frame that randomly assigns one of the four elements in the vector "months" to each observation. Start your code with the command "set.seed(123)".

```{r}
set.seed(123)
dat[,5] <- sample(months, size=nrow(dat), replace=T)
colnames(dat) <- c("x", "x.plus", "x.multiply", "dev", "month")
```

d) Using for loops, compute the means of the first (x) and fourth columns (dev) separately for each month. That is, you will compute eight different values (i.e., first column mean for January, fourth column mean for January, first column mean for February, fourth column mean for February, and so forth). Repeat this exercise with medians. 

```{r}
mat.means <- matrix(nrow=4, ncol=2)
mat.medians <- matrix(nrow=4, ncol=2)

for(i in 1:length(months)){
  subset <- subset(dat[c("x", "dev")], dat$month==months[i])
  mat.means[i,1] <- mean(subset[,1], na.rm=T)
  mat.means[i,2] <- mean(subset[,2], na.rm=T)
  mat.medians[i,1] <- median(subset[,1], na.rm=T)
  mat.medians[i,2] <- median(subset[,2], na.rm=T)
}

rownames(mat.means) <- months
rownames(mat.medians) <- months

colnames(mat.means) <- c("x", "dev")
colnames(mat.medians) <- c("x", "dev")

mat.means
mat.medians
```

e) **BONUS**: We have not covered plotting in the first R lab yet. Therefore, this problem is completely optional and please do not worry if you decide not to solve it. However, the help function or the internet will be helpful to solve this problem. Use the "hist()" function to produce a histogram of the dev variable. Next, produce a scatterplot of the x.multiply (vertical axis) against the x variable (horizontal axis). For both plots, add an informative plot title and label both x and y axes.

```{r}
# Histogram
hist(dat$dev, main="Histogram of Average Absolute Deviation \n from the Mean", 
     xlab="Deviation", ylab="Frequency")

# Scatterplot
plot(dat$x, dat$x.multiply, main=expression(X^2~"on X"), ylab=expression(X^2), xlab="X")
```


### Problem 2: Writing Functions


a) Load the R Data Frame "dta.Rdata". Figure out what this object is called using the ls() function, use head() to look at the data. It may be helpful to remove other objects in your workspace using rm(list=ls()).

```{r}
rm(list=ls())
load("../data/dta.Rdata")
ls()
head(dta)
```

b) Write a function called "average" to take the mean of the variable "independent.variable" in the data frame. This function should take a data frame as an input and return the average value of "independent.variable" in the data frame it is supplied with. Do **not** use R's "mean" function to perform this calculation. Instead, write your own function to do so. What is the mean of the "independent.variable" column in this data frame? Does this value correspond with the result you obtain when using R's canned "mean" function? Use a logic statement to answer this last question.

```{r}
## Function
average <- function(df){
  out <- sum(df$independent.variable)/length(df$independent.variable)
  return(out)
}

## Mean using the function
average(df=dta)

```

##### Does the value that the "average" function produces correspond with the output from R's "mean" function?

```{r}
average(df=dta) == mean(dta$independent.variable)
```

c) Write a second function, "average.two.obs" to take the mean of the variable "independent.variable" based on only the first two observations in the data frame that is passed to it. This function should return the average value of "independent.variable" based on just these two observations. What is the resulting estimate of the mean you obtain when you use this function on the data frame? Does this value correspond with the result you obtain when using R's canned "mean" function? Use a logic statement to answer this last question.

```{r}
## Function
average.two.obs <- function(df){
  sub <- df[1:2,]
  out <- sum(sub$independent.variable) / length(sub$independent.variable)
  return(out)
}

## Estimated Mean
average.two.obs(df=dta)
```

##### Does the value that the "average.two.obs" function produces correspond with the output from R's "mean" function?

```{r}
average.two.obs(df=dta) == mean(dta$independent.variable)
```


### Problem 3: Combining Loops and Functions to Evaluate Consistency of Estimators

Set a seed of "8989". Use the following code to start the answer the subsequent questions, which draw upon the functions written in Problem 2.

```{r}
#Load Data
load("../data/dta.Rdata")

## set seed
set.seed(8989)
```


a) Write a loop that applies the "average" and "average.two.obs" functions to an increasingly large portion of the overall dataframe. Specifically, apply your functions to every sample size between 10 and 500, in increments of 10. That is, start by applying the functions to the first ten rows of the dataframe only and save the resulting averages. Then apply the functions to the first 20 rows and save the resulting averages, and so forth until you include the first 500 rows of the data frame. Display the head of both vectors created by the loop:


```{r}
## Define row length
row.length <- seq(from= 10, to = 500, by = 10)

## Create two empty vectors that will receive the output of the for loop
averages.part2 <- NA
averages.two.obs.part2 <- NA

## For loop
for(i in 1:length(row.length)){
    sub <- 1:row.length[i]
    averages.part2[i] <- average(dta[sub, ])
    averages.two.obs.part2[i] <- average.two.obs(dta[sub, ])
}

## Display Head Vector 1
head(averages.part2)

## Display Head Vector 2
head(averages.two.obs.part2)
```

b) Plot the resulting information. On the x-axis plot the sample size used to estimate the mean (e.g., the first mean will be at 10 on the x-axis, the second at 20, and so on). On the y-axis plot the resulting estimate. Rather than points, plot these values as a solid line. On the same plot, graph the estimates from "average.two.obs" on the same plot using a line of a different color. Produce a title and label each of the axes in your plot. How does this compare to the true value we were supposed to get, 25? Draw a horizontal line at 25. 

```{r}
plot(x=row.length, y=averages.part2, type='l', ylim=c(18,30), 
     ylab="Estimate", xlab="Sample Size", main="Estimates of Mean as Sample Size Increases")
lines(x=row.length, y=averages.two.obs.part2, col="red")
abline(h=25, lwd=3)
```

c) If an estimator gets closer to the value it is trying to estimate as the sample it is applied to grows in size, we call it consistent. Do either of these estimators appear to be consistent based on your graph?


##### The average estimator appears to be consistent because it tends toward the correct value, while average.two.obs does not (it always takes the same value, no matter how much data is used).


---

## Part 2



### Introduction

We have already encountered cursory examples of ordinary least squares (OLS) regression. It turns out that as long as our data matrix, $X$ is full rank, then the OLS estimator for $\beta$, the vector of slopes of the best-fit line, can be written in matrix form as follows:

$\hat{\beta} = (X'X)^{-1}X'Y$. In words: the inverse of the $X'X$ matrix, multiplied by the transpose of the $X$ matrix, multipled by $Y$.

Typically, $X$ is a matrix of predictor variables that includes a constant (a column of 1's), and $Y$ is a vector of outcomes. The object $(X'X)^{-1}X'Y$ is a $k x 1$ matrix of estimated coefficients---one for each unknown in the model, including the constant. So for example, if we want to estimate the following model:

$$Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$

where $X_1$ and $X_2$ are both columns in the matrix $X$, $(X'X)^{-1}X'Y$ would return a vector of $k=3$ coefficient estimates: $\hat{\alpha}$, $\hat{\beta}_1$ and $\hat{\beta}_2$ (we use "hats" here to convey that these are estimates and not the true parameter values; an estimate of $\epsilon$, a vector of errors, is not included in this output vector).

This assignment will focus on the mechanics of applying this estimator in R using matrix operations. In short, you are going to write your own OLS function!

To do this, we are going to have you read in a real, uncleaned data set, clean it as needed, and then apply your OLS function to estimate an OLS model. After working through this, much of the mechanics in 450A should seem slightly less scary, or at least have a ring of familiarity when they arrive. This will also give you insight as to what is going on "under the hood" when we use canned OLS functions in R such as lm().



### Problem 1: Pre-processing data

1) Read in the "commoncontent2012.RData" data, which is the raw 2012 CCES data. Relabel this data frame "dd". Check the dimensions and examine the first few rows of the data. You will notice that all of these variables do not have intuitive names, and some of them contain weird values. So next we will need to pre-process these data.

```{r}
rm(list=ls())
load("../data/commoncontent2012.RData")
dd <- x
dim(x)
```

2) We first want to identify the party of the respondents in these data. Let's make a new variable (i.e., a new column in our data frame) called "dem" that takes a 1 if a respondent self-identified as a Democrat (see pid3), or said they leaned toward the Democratic party (see pid7others) in a follow-up question, and a 0 otherwise. Do the same thing for Republicans using the same two variables. Hint: the functions table() and class() are useful for determining which values a variable contains and what type of vector it is, respectively.
```{r}
## What values does pid3 take?
table(dd$pid3)

## What values does pid7others take? 
table(dd$pid7others)

## Coding dummy variable for Democrats
dd$dem <- 0
dd$dem[dd$pid3 == "Democrat"] <- 1
dd$dem[dd$pid7others == "Lean Democrat"] <- 1

## Coding dummy variable for Republicans
dd$rep <- 0
dd$rep[dd$pid3 == "Republican"] <- 1
dd$rep[dd$pid7others == "Lean Republican"] <- 1
```

3) For those labeled  "Skipped" or "Not asked" on pid3, code them as NA. For those labeled "Not sure", "Skipped" or "Not asked" on pid7others, code them as NA as well. How many respondents that identify as Democrats and Republicans, respectively, do you identify in the dataset?
```{r}
## Democrats
dd$dem[dd$pid3 == "Skipped" | dd$pid3 == "Not Asked"] <- NA
dd$dem[dd$pid7others == "Not Sure" | dd$pid7others == "Skipped" | dd$pid7others == 
    "Not Asked"] <- NA

## Republicans
dd$rep[dd$pid3 == "Skipped" | dd$pid3 == "Not Asked"] <- NA
dd$rep[dd$pid7others == "Not Sure" | dd$pid7others == "Skipped" | dd$pid7others == 
    "Not Asked"] <- NA

## How many Democrats and Republicans?
table(dd$dem)
table(dd$rep)
```

4) Make a new column in dd, age, that is a numeric equal to the respondent's age in years. Do this using the variable birthyr, which is a factor vector that conveys the respondent's year of birth. You may need to change the class of birthyr in order to accomplish this. Note that this survey was conducted in 2012. What is the mean age of all respondents in the dataset?
```{r}
## Class of birthyr is factor:
class(dd$birthyr)

## Creating a new variable, birthyr2, which is numeric
dd$birthyr <- as.character(dd$birthyr)
dd$birthyr <- as.numeric(dd$birthyr)

## Generating the age variable
dd$age <- 2012 - dd$birthyr

## Mean age
mean(dd$age, na.rm=T)
```

5) Create a new column---"female"---that equals 1 if the respondent is a female and 0 if the respondent is a male using the variable "gender". What percent of the respondents is female?
```{r}
## What values does the gender variable take?
table(dd$gender, exclude = NULL)

## Generating the variable "female"
dd$female <- NA
dd$female[dd$gender == "Male"] <- 0
dd$female[dd$gender == "Female"] <- 1
table(dd$female)

## Percent of respondents that are female
mean(dd$female)
```

6) Using the variable educ, create a column, BA, that equals 1 if the respondent has a Bachelor's Degree or higher, and 0 otherwise. Be mindful of the class of the original variable. Make sure BA ends up as numeric. How many respondents hold at least a B.A.?
```{r}
## What values does the educ variable take?
table(dd$educ)

## Creating the BA variable
educ <- as.character(dd$educ)
dd$BA <- 0
dd$BA[educ == "4-year" | educ == "Post-grad"] <- 1
table(dd$BA)
```

7) Construct a variable obama, that equals 1 if the respondent voted for President Obama, 0 if they voted for someone else, and NA if the did not vote or did not answer the question or are not sure. Use the variable CC410a. What percent of respondents voted for someone *other than* President Obama?
```{r}
## Creating the variable
dd$obama <- NA
dd$obama[dd$CC410a == "Mitt Romney (Republican)" | dd$CC410a == "Other"] <- 0
dd$obama[dd$CC410a == "Barack Obama (Democratic)"] <- 1

## Percent of respondents who did not vote for Obama?
1 - mean(dd$obama, na.rm=T)
```


### Problem 2: Writing an OLS Function using Matrix Algebra

1) Construct a matrix called X where the columns are: a vector of 1's of the same length as the variables you just created, as well as the dem, rep, female, age, and BA variables---*in that order*. Make sure the column names remain the same after constructing the matrix; label the column of 1's "constant".
```{r}
X <- cbind(rep(1, nrow(dd)), dd$dem, dd$rep, dd$female, dd$age, dd$BA)
colnames(X) <- c("constant", "dem", "rep", "female", "age", "BA")
```

2) Construct a *matrix* Y that is just one column, obama. Again, make sure the column name remains the same.
```{r}
Y <- as.matrix(dd$obama)
colnames(Y) <- "obama"
```

3) Use your X and Y matrices to implement the OLS estimator---$(X'X)^{-1}X'Y$---to estimate the unknown parameters (the constant term and the betas) in the following regression:

$$obama = constant + \beta_1\text{dem} + \beta_2\text{rep} + \beta_3\text{female} + \beta_4\text{age}  +  \beta_5\text{ba} +\epsilon$$

```{r}
## Need to drop incomplete cases
X.new <- X[!is.na(Y),]
Y.new <- Y[!is.na(Y)]

## Estimating the parameters
betas <- solve(t(X.new) %*% X.new) %*% (t(X.new) %*% Y.new)
betas
```

4) Using what we know about how to write functions and how to perform matrix operations in R, write a function called "OLS.est" that takes as arguments a data frame, a character vector of the names of independent variables, and a character vector with the name of the dependent variable. Have the function subset the data frame to the variables of interest, compute the OLS estimator $(X'X)^{-1}X'Y$, and return a kx1 matrix of estimated coefficients called "beta.hat". Make sure that by default the function renders the first column of X a constant vector of 1's, and give this column the name "(Intercept)" (the constant is often referred to as the intercept, and it is good to practice working with column names). Note: if an observation (a row) is missing on either an X variable or Y, that entire row cannot be included in the OLS model and must be deleted. Make sure your function accounts for this fact. Also, recall that the first column of the matrix of independent variables should be the constant term. You will have to add it inside the function.
```{r}
OLS.est <- function(df, dv.name, iv.names) {
    dd.temp <- df[, c(iv.names, dv.name)]  ## subset the data frame to the variables you wish to include in the model
    dd.temp <- na.omit(dd.temp)  ## drop missing values
    colnames(dd.temp) <- c(iv.names, dv.name)
    Y <- as.matrix(dd.temp[, dv.name])  ## separate out Y
    X <- cbind(1, dd.temp[, iv.names])  ## separate out X, adding a constant term
    colnames(X) <- c("(Intercept)", iv.names)
    X <- as.matrix(X) ## X has to be a matrix
    beta.hat <- solve(t(X) %*% X) %*% t(X) %*% Y ## Implementing the OLS estimator
    results <- list(beta.hat)
    names(results) <- c("beta.hat")
    return(results)
}
```


### Problem 3: Applying your function to actual data

1) Apply your new function to the data frame "dd" (that is, the whole CCES data frame that you pre-processed in Problem 1. Do not alter it any further prior to passing it to the function and have the subsetting to relevant variables occur within the function). Again, estimate the unknown parameters (the constant term and the betas) in the following regression:

$$obama = constant + \beta_1\text{dem} + \beta_2\text{rep} + \beta_3\text{female} + \beta_4\text{age}  +  \beta_5\text{ba} +\epsilon$$

```{r}
OLS.est(df = dd, dv.name = c("obama"), iv.names = c("dem", "rep", "female", "age", "BA"))
```

2) Confirm these estimates are correct by estimating the same regression using the lm() function. Use the ? command or search online for how to use this function. Examples abound.
```{r}
summary(lm(obama ~ dem + rep + female + age + BA, data=dd))
```


### Problem 4

You will notice that the summary output of the lm() function contained standard errors. These are estimates of the standard deviations that distributions of these coefficients would possess if we took many samples of data and estimated these models many times. In other words, they are estimates of the variability in our estimates of these coefficients given this sample of data. Let's use matrix operations to estimate these standard errors.

1) Revise your OLS.est function to calculate an additional object, a one-column matrix "e" that is equal to $Y - X \hat{\beta}$. This is a vector of residuals, which are estimates of the errors in the model. Still working inside the function, generate a new object which is equal to the sum of the squares of each of the elements in "e", which should be a constant. Call this new object e.2 and make sure it is of class numeric. Have the function return beta.hat and e.2. Since you are returning multiple objects, have the output of the function be a list. Use your function to compute the same regression model as before.
```{r}
## Updated function
OLS.est <- function(df, dv.name, iv.names) {
    ## subset the data frame to the variables you wish to include in the model
    dd.temp <- df[, c(iv.names, dv.name)]  
    dd.temp <- na.omit(dd.temp)  ## drop missing values
    colnames(dd.temp) <- c(iv.names, dv.name)
    Y <- as.matrix(dd.temp[, dv.name])  ## separate out Y
    X <- cbind(1, dd.temp[, iv.names])  ## separate out X, adding a constant term
    colnames(X) <- c("(Intercept)", iv.names)
    X <- as.matrix(X) ## X has to be a matrix
    beta.hat <- solve(t(X) %*% X) %*% t(X) %*% Y ## Implementing the OLS estimator
    e <- Y - X %*% beta.hat ## Calculating residuals
    e.2 <- as.numeric(t(e) %*% e) ## Calculating the sum of squared residuals
    results <- list(beta.hat, e.2) ## output as list
    names(results) <- c("beta.hat", "e.2")
    return(results)
}

## Applying the function as before
OLS.est(df = dd, dv.name = c("obama"), iv.names = c("dem", "rep", "female", "age", "BA"))
```

2) Revise the function yet again to output a new $kxk$ matrix, "var.cov", that is equal to $\frac{e.2}{n-k}*(X'X)^{-1}$, where $n$ is the number of observations that were included in the regression, $k$ is the number of estimated parameters, including the constant, and $X$ is the matrix of independent variables included in the regression. 
```{r}
## Updated function
OLS.est <- function(df, dv.name, iv.names) {
    ## subset the data frame to the variables you wish to include in the model
    dd.temp <- df[, c(iv.names, dv.name)]  
    dd.temp <- na.omit(dd.temp)  ## drop missing values
    colnames(dd.temp) <- c(iv.names, dv.name)
    Y <- as.matrix(dd.temp[, dv.name])  ## separate out Y
    X <- cbind(1, dd.temp[, iv.names])  ## separate out X, adding a constant term
    colnames(X) <- c("(Intercept)", iv.names)
    X <- as.matrix(X) ## X has to be a matrix
    beta.hat <- solve(t(X) %*% X) %*% t(X) %*% Y ## Implementing the OLS estimator
    e <- Y - X %*% beta.hat ## Calculating residuals
    e.2 <- as.numeric(t(e) %*% e) ## Calculating the sum of squared residuals
    var.cov <- (e.2/(nrow(X) - ncol(X))) * solve((t(X) %*% X)) ## variance-covariance matrix    
    results <- list(beta.hat, e.2, var.cov)
    names(results) <- c("beta.hat", "e.2", "var.cov")
    return(results)
}

## Applying the function as before
OLS.est(df = dd, dv.name = c("obama"), iv.names = c("dem", "rep", "female", "age", "BA"))
```

3) Revise your function one last time to output an additional object, a vector called "ses", that is equal to the square root of the diagonal elements of var.cov (you may find diag() helpful for this question). So now, the function should output beta.hat, e.2, var.cov and ses in a list. Compare the ses vector to the standard errors estimated by lm() above. Are they the same?
```{r}
## Updated function
OLS.est <- function(df, dv.name, iv.names) {
    ## subset the data frame to the variables you wish to include in the model
    dd.temp <- df[, c(iv.names, dv.name)]  
    dd.temp <- na.omit(dd.temp)  ## drop missing values
    colnames(dd.temp) <- c(iv.names, dv.name)
    Y <- as.matrix(dd.temp[, dv.name])  ## separate out Y
    X <- cbind(1, dd.temp[, iv.names])  ## separate out X, adding a constant term
    colnames(X) <- c("(Intercept)", iv.names)
    X <- as.matrix(X) ## X has to be a matrix
    beta.hat <- solve(t(X) %*% X) %*% t(X) %*% Y ## Implementing the OLS estimator
    e <- Y - X %*% beta.hat ## Calculating residuals
    e.2 <- as.numeric(t(e) %*% e) ## Calculating the sum of squared residuals
    var.cov <- (e.2/(nrow(X) - ncol(X))) * solve((t(X) %*% X)) ## variance-covariance matrix    
    ses <- sqrt(diag(var.cov)) # computing standard errors
    results <- list(beta.hat, e.2, var.cov, ses)
    names(results) <- c("beta.hat", "e.2", "var.cov", "ses")
    return(results)
}

## Applying the function as before
OLS.est(df = dd, dv.name = c("obama"), iv.names = c("dem", "rep", "female", "age", "BA"))
```


### BONUS: For your own enjoyment

Note, if you find this daunting, don't worry.  450A will cover this in depth.

1) Interpret the coefficients on BA and female, respectively.

* There are many sentences that woud be correct, but something like:

* “Holding party, age and gender constant, possessing a 4-year college degree or higher is associated with an increase of about 3.8 percentage points in the probability of voting for Obama, on average.”

* “Holding party, age and education constant, being female is associated with an increase of about 3.5 percentage points in the probability of voting for Obama, on average.”

* Some social scientists would caution you to be careful not to use causal language here like “affects” or “drives” or “leads to” without making very clear the assumptions that would need to hold in order for that assumption to be correct. More on this in 450A and 450B.

2) What is the predicted value of Y (whether or not someone voted for Obama) for an 95-year-old Democrat who is female and went to college? What about a 50-year-old Republican who is male and went to college? Hint: refer back to the equation for the model we estimated, and note that you now have estimated values for the unknown parameters.

```{r}
## Store the estimated coefficients in a vector
coef <- OLS.est(df = dd, dv.name = c("obama"), 
                iv.names = c("dem", "rep", "female", "age", "BA"))$beta.hat

## Probability for the 95-year-old Democrat who is female and went to college:
coef[1] + 1 * coef[2] + 0 * coef[3] + 1 * coef[4] + 95 * coef[5] + 1 * coef[6]

## Probability for the 50-year-old Republican who is male and went to college:
coef[1] + 0 * coef[2] + 1 * coef[3] + 0 * coef[4] + 50* coef[5] + 1 * coef[6]
```

3) For both predictions, do such people exist in the data set? If so, how many?
```{r}
## nobody who fulfills the first set of conditions:
table(dd$obama[dd$dem == 1 & dd$age == 95 & dd$female == 1 & dd$BA == 1])

## 45 people who fulfill the second set of conditions
##--4 (41) of whom voted (did not vote) for President Obama
table(dd$obama[dd$rep == 1 & dd$age == 50 & dd$female == 0 & dd$BA == 1])
```
